#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jul  4 11:00:30 2018

@author: miri-o
"""

from miris_tools import plot_confusion_matrix

import pandas as pd
import numpy as np
import time
import seaborn as sns
import matplotlib.pyplot as plt
import re


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

from miris_tools.kmeans import unsupervised_clustering

if __name__ == '__main__':
        
    path = '/media/miri-o/Documents/'
    n = 30 #Number of clusters per step
    log_file_path = '/media/miri-o/Documents/logs/kMeans_Clustering_HCV_full.log'
    log_file = open(log_file_path, 'w')
    
    if False: # trying the new clustering class
        infile = pd.read_csv(path+'TSNE_results_50k_HCV_data_Celiac_model.csv')
        X = np.column_stack((infile['dim1'], infile['dim2']))
        X = pd.DataFrame(X)
        us_clust = unsupervised_clustering(debug_mode=False)
        kmeans = us_clust.fit(X)
        us_clust.visualize()  
        us_clust.save_clusters_to_file(path+'TSNE_results_50k_HCV_data_Celiac_model.csv')
        
        
    if False:
        filename = 'HCV_vectors_for_PCA_20180529.csv'
        fullfile = pd.read_csv(path+filename, delimiter = '\t', index_col = 0)
        print('~~~~\nFile:'+filename+'\n')
        N = 50000
        sampled_indexes = np.random.choice(fullfile.index, N, replace=False)
        us_clust = unsupervised_clustering(debug_mode=False)
        X = fullfile.loc[sorted(sampled_indexes)]
        X.index = range(len(X))
        kmeans = us_clust.fit(X)
        
    if False:
        filename = 'HCV_vectors_for_PCA_20180529.csv'
        fullfile = pd.read_csv(path+filename, delimiter = '\t', index_col = 0)
        print('~~~~\nFile:'+filename+'\n')
        us_clust = unsupervised_clustering()
        X = fullfile.copy()
        X.index = range(len(X))
        kmeans = us_clust.fit(X)
    # Clustering for 2D tSNE including visualiztion:
    if False: 
        infile = pd.read_csv(path+'TSNE_results_50k_HCV_data_Celiac_model.csv')
        X = np.column_stack((infile['dim1'], infile['dim2']))
        km = kmeans.kMeans_clustering(X, n, plot = True)
        for i in range(km.n_clusters):
            print('Cluster number: ' + str(i))
            if np.sum(km.labels_==i)>10:
                #sc = plot_embedding_with_properties(infile['dim1'][km.labels_==i], infile['dim2'][km.labels_==i], km.labels_[km.labels_==i], title = ('cluster #'+str(i)))
                X_small = np.column_stack((infile['dim1'][km.labels_==i], infile['dim2'][km.labels_==i]))                                   
                print('~~~ Starting clustering of N='+str(len(X)))
                t0 = time.time()
                y_hat_small = kmeans.kMeans_clustering(X_small, n, True)
                print('finished, time: {:.4} sec'.format(time.time()-t0))        
    
    if False: #100 dimensions clustering      
        filename = 'HCV_vectors_for_PCA_20180529.csv'
        fullfile = pd.read_csv(path+filename, delimiter = '\t', index_col = 0)
        print('~~~~\nFile:'+filename+'\n')
        log_file.write('~~~~\nFile:'+filename+'\n')
        print('Starting initial clustering with n='+str(n)+'\n')
        log_file.write('Starting initial clustering with n='+str(n)+'\n')
        t0 = time.time()
    #    N = 500000
    #    sampled_indexes = np.random.choice(range(len(fullfile)), N, replace=False)
        km = kmeans.kMeans_clustering(fullfile.values, n, plot = False)
        labels = {}
        clust_index = 1
        print('finished, time: {:.4} sec'.format(time.time()-t0))
        log_file.write('finished, time: {:.4} sec'.format(time.time()-t0))
        print('Starting SUB-CLUSTERING clustering with n='+str(n)+'\n')
        log_file.write('Starting SUB-CLUSTERING clustering with n='+str(n)+'\n')
        for i in range(km.n_clusters):
            print('Cluster number: {}, initial cluster size: {}'.format(i,np.sum(km.labels_==i)))
            if np.sum(km.labels_==i)>10:
                #sc = plot_embedding_with_properties(infile['dim1'][km.labels_==i], infile['dim2'][km.labels_==i], km.labels_[km.labels_==i], title = ('cluster #'+str(i)))
                original_indexes, = np.where(km.labels_==i)
                indexes_to_chose = fullfile.index[original_indexes] #fullfile has some missing indexes
                X_small = fullfile.loc[indexes_to_chose] 
                print('~~~ Starting clustering of N='+str(len(X_small)))
                log_file.write('~~~ Starting clustering of N='+str(len(X_small)))
                t0 = time.time()
                y_hat_small = kmeans.kMeans_clustering(X_small, n, False)
                for j in range(y_hat_small.n_clusters):
                    labels[clust_index] = indexes_to_chose[y_hat_small.labels_==j]
                    print('Cluster number {}:, {} points'.format(clust_index, len(labels[clust_index])))
                    log_file.write('Cluster number {}:, {} points'.format(clust_index, len(labels[clust_index])))
                    clust_index +=1                   
                print('~~~ finished, time: {:.4} sec'.format(time.time()-t0))  
                log_file.write('~~~ finished, time: {:.4} sec'.format(time.time()-t0))  
        np.save('HCV_labels.npy', labels)
        inv_labels = {}
        for k in labels: #go over every cluster and inverse the values
            indexes = labels[k].values
            for index in indexes:
                inv_labels[index] = k
                
        # insert the clusters to data file         
        datafile = pd.read_csv(path+'HCV_data_for_PCA_20180529.csv', delimiter='\t')
        datafile['original index'] = fullfile.index
        datafile['cluster'] = [inv_labels[i] for i in datafile['original index']]
        log_file.close()
        
        
        # Build a feature table where each column is a cluster and each raw is a subject, 
        # count the number of sequnces of each subject in each cluster

        features_table = pd.DataFrame(0, index=pd.unique(datafile['SUBJECT']), columns=labels)
        for index, row in datafile.iterrows():
            features_table.loc[row.SUBJECT, row.cluster] +=1
        
        # Normlize by raw
        normlized_features_table = features_table.div(features_table.sum(axis=1), axis=0)

        fig, ax = plt.subplots(figsize=(15,6)) 
        sns.heatmap(normlized_features_table, cmap='viridis')
        
        ### Additional filtering - filter clusters with low subject diversity, i.e. X% of cell originated from one subject
        all_features_sum = normlized_features_table.sum(axis=0)
        all_features_max = normlized_features_table.max(axis=0)
        max_feature_precentage = all_features_max*100/all_features_sum
        max_feature_precentage = pd.DataFrame(max_feature_precentage, columns=['Precentage'])
        cut_off_TH = 95
        to_drop = pd.DataFrame([(center, value) for (center, value) in zip(max_feature_precentage.index, max_feature_precentage.Precentage) if value>cut_off_TH], columns= ['Cluster', 'Precentage_of_top_feature'])
        print(to_drop)
        
        # droping the above centers
        filtered_feature_table = normlized_features_table.drop(labels=to_drop.Cluster, axis = 1)
        print('Filtering {} columns where maximal feature precentage exceeds allowed TH, Centers: {}, Precentages: {}'.
              format(len(to_drop),list(to_drop.Cluster), list(to_drop.Precentage_of_top_feature)))

        # Let's do some machine learning
        
        # Logistic regression for al 3 classes
        y = [re.split('\d', condition)[0] for condition in filtered_feature_table.index]
        
        logmodel = LogisticRegression(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,
                   intercept_scaling=1, max_iter=100, multi_class='multinomial', n_jobs=1,
                   penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,
                   verbose=0, warm_start=True)
        predictions_all =[]
        actual_all = []
        coefs = 0
        
        for i in range(100):
            X_train, X_test, y_train, y_test = train_test_split(filtered_feature_table, y, test_size=0.1)
            logmodel.fit(X_train,y_train)
            predictions_all.extend(list(logmodel.predict(X_test)))
            actual_all.extend(y_test)
            #print('True labels: '+ str(y_test) + ' Predicted labels: ' + str(predictions))
            #print(classification_report(y_test,predictions))
            coefs = coefs + logmodel.coef_
            
        # Compute confusion matrix
        cnf_matrix = confusion_matrix(actual_all, predictions_all)
        np.set_printoptions(precision=2)
        
        # # Plot non-normalized confusion matrix
        # plt.figure()
        # plot_confusion_matrix(cnf_matrix, classes=['CI','SC'],
        #                       title='Confusion matrix, without normalization')
        
        # Plot normalized confusion matrix

        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['C','CI','SC'], normalize=True,
                              title='Normalized confusion matrix'+ ' score: ' + str(accuracy_score(actual_all, predictions_all)))
        #plt.savefig('Articles/plots/HCV_3_classes_LR.pdf')
        

        print('score: ' + str(accuracy_score(actual_all, predictions_all)))
        
        # Decision tree
        cond_to_bin = {'C':0, 'CI':1, 'SC':2}
        y_bin = [cond_to_bin[x] for x in y]   
        
        model = tree.DecisionTreeClassifier(max_depth=20)
    
        predictions_all =[]
        actual_all = []
    
        for i in range(1000):
            X_train, X_test, y_train, y_test = train_test_split(filtered_feature_table, y_bin, test_size=0.2)
            model.fit(X_train,y_train)
            predictions_all.extend(list(model.predict(X_test)))
            actual_all.extend(y_test)
           
        cnf_matrix = confusion_matrix(actual_all, predictions_all)
        np.set_printoptions(precision=2)
        
        # # Plot non-normalized confusion matrix
        # plt.figure()
        # plot_confusion_matrix(cnf_matrix, classes=['CI','SC'],
        #                       title='Confusion matrix, without normalization')
        
        # Plot normalized confusion matrix

        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['C','CI','SC'], normalize=True,
                              title='Decision Tree, 3 classes'+' score: ' + str(accuracy_score(actual_all, predictions_all)))

        print('score: ' + str(accuracy_score(actual_all, predictions_all)))
        
        ### only on 2 classes (Decision tree):
        SC_CI_indexes = [i for (i, name) in enumerate(y) if name!='C']
        CI_SC_features = filtered_feature_table.iloc[SC_CI_indexes,:]
        y_CI_SC = [y[i] for i in SC_CI_indexes]
        cond_to_bin = {'CI':0, 'SC':1}
        y_CI_SC_bin = [cond_to_bin[x] for x in y_CI_SC]   
        
        model = tree.DecisionTreeRegressor(max_depth=30)

        predictions_all =[]
        actual_all = []
        
        for i in range(1000):
            X_train, X_test, y_train, y_test = train_test_split(CI_SC_features, y_CI_SC_bin, test_size=0.1)
            model.fit(X_train,y_train)
            predictions_all.extend(list(model.predict(X_test)))
            actual_all.extend(y_test)
            #print('True labels: '+ str(y_test) + ' Predicted labels: ' + str(predictions))
            #print(classification_report(y_test,predictions))
            
        # Compute confusion matrix
        cnf_matrix = confusion_matrix(actual_all, predictions_all)
        np.set_printoptions(precision=2)

        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['CI','SC'], normalize=True,
                              title='Normalized confusion matrix' + ' score: ' + str(accuracy_score(actual_all, predictions_all)))
        
        print('score: ' + str(accuracy_score(actual_all, predictions_all)))
        
        ## Random forest
        model = RandomForestClassifier(random_state=0)
        
        predictions_all =[]
        actual_all = []
        for i in range(1000):
            X_train, X_test, y_train, y_test = train_test_split(CI_SC_features, y_CI_SC_bin, test_size=0.1)
            model.fit(X_train,y_train)
            predictions_all.extend(list(model.predict(X_test)))
            actual_all.extend(y_test)
            
        cnf_matrix = confusion_matrix(actual_all, predictions_all)
        np.set_printoptions(precision=2)

        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['CI','SC'], normalize=True,
                              title='Normalized confusion matrix' + ' score: ' + str(accuracy_score(actual_all, predictions_all)))
        
        print('score: ' + str(accuracy_score(actual_all, predictions_all)))            

    if False:
        filename = 'HCV_vectors_after_PCA_train_set.csv'
        train_data = pd.read_csv(path+filename, sep = '\t')
        print('~~~~\nFile:'+filename+'\n')
        log_file.write('~~~~\nFile:'+filename+'\n')
        print('Starting initial clustering with n='+str(n)+'\n')
        log_file.write('Starting initial clustering with n='+str(n)+'\n')
        t0 = time.time()
    #    N = 500000
    #    sampled_indexes = np.random.choice(range(len(fullfile)), N, replace=False)
        km = kmeans.kMeans_clustering(train_data.iloc[:, :-1], n, plot = False)
        labels = {}
        clust_index = 1
        print('finished, time: {:.4} sec'.format(time.time()-t0))
        log_file.write('finished, time: {:.4} sec'.format(time.time()-t0))
        print('Starting SUB-CLUSTERING clustering with n='+str(n)+'\n')
        log_file.write('Starting SUB-CLUSTERING clustering with n='+str(n)+'\n')
        for i in range(km.n_clusters):
            print('Cluster number: {}, initial cluster size: {}'.format(i,np.sum(km.labels_==i)))
            if np.sum(km.labels_==i)>10:
                #sc = plot_embedding_with_properties(infile['dim1'][km.labels_==i], infile['dim2'][km.labels_==i], km.labels_[km.labels_==i], title = ('cluster #'+str(i)))
                original_indexes, = np.where(km.labels_==i)
                X_small = train_data.loc[original_indexes] 
                print('~~~ Starting clustering of N='+str(len(X_small)))
                log_file.write('~~~ Starting clustering of N='+str(len(X_small)))
                t0 = time.time()
                y_hat_small = kmeans.kMeans_clustering(X_small, n, False)
                for j in range(y_hat_small.n_clusters):
                    labels[clust_index] = original_indexes[y_hat_small.labels_==j]
                    print('Cluster number {}:, {} points'.format(clust_index, len(labels[clust_index])))
                    log_file.write('Cluster number {}:, {} points'.format(clust_index, len(labels[clust_index])))
                    clust_index +=1                   
                print('~~~ finished, time: {:.4} sec'.format(time.time()-t0))  
                log_file.write('~~~ finished, time: {:.4} sec'.format(time.time()-t0))
        